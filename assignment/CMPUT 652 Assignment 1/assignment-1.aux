\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Consider the supervised regression problem of predicting an output $Y$ from a vector input $\bf  x$ using a linear function with a vector parameter $\bf  w$. We found in the lecture that the Mean Squared Error (MSE) $\operatorname  {E} \left [ \left ( Y - {\bf  x}^\top {\bf  w}\right )^2 \right ]$ is a reasonable objective for this problem. Derive the Least-Squares (LS) method, defined below, from MSE (5 points). }{2}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Continuing with the supervised regression problem, derive the Stochastic Gradient Descent (SGD) method, defined below, from MSE (5 points). }{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ According to the strong law of large numbers, sample mean is a consistent estimator of the expected value (See Lemma 3, pg 18, of \href  {https://era.library.ualberta.ca/files/cbc386j54q/Mahmood_Ashique_201709_PhD.pdf}{Mahmood 2017}). Use this fact to argue that the sample MSE $\frac  {1}{t} \DOTSB \sum@ \slimits@ _{k=1}^t \left ( Y_k - {\bf  x}_k^\top {\bf  w}\right )^2$ objective is a consistent estimator of MSE (5 points). }{3}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Similarly, argue that LS is a consistent estimator of the solution to MSE (5 points). }{3}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Establish a relationship between MSE and the following: $\operatorname  {E} _{{\bf  x}}\left [ \left ( \operatorname  {E} _{Y|{{\bf  x}}}\left [ Y\right ] - {\bf  x}^\top {\bf  w}\right )^2 \right ]$. Note that MSE can be written as $\operatorname  {E} \left [ (Y - {\bf  x}^\top {\bf  w})^2 \right ] = \operatorname  {E} _{{\bf  x}}\operatorname  {E} _{Y|{{\bf  x}}}\left [ (Y - {\bf  x}^\top {\bf  w})^2 \right ]$. Also, the expectations $\operatorname  {E} _{X}$ and $\operatorname  {E} _{Y|X}$ can be elaborated as $\operatorname  {E} _{X} f(X) = \DOTSI \intop \ilimits@ _{\mathcal  {X}} p^*(x) f(x) dx$ and $E_{Y|X} f(Y) = \DOTSI \intop \ilimits@ _{\mathcal  {Y}} p^*(y|x) f(y) dy$ (5 points). }{4}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Objective functions serve many purposes. They bring specificity to the problem formulation, provide a criteria for evaluation and can be used as a basis for deriving new algorithms. Accordingly, the same problem may have different associated objectives. For example, the sample MSE is widely used to evaluate the performance of a learning method, but not all methods ensue from MSE. Consider the following objective $\operatorname  {MSE}_{\operatorname  L2}$, which is a slight modification to MSE: $\operatorname  {MSE}_{\operatorname  L2}({\bf  w}) = \operatorname  {E} \left [ \left ( Y - {\bf  x}^\top {\bf  w}\right )^2 + \lambda {\bf  w}^\top {\bf  w}\right ]$. Derive the following method know as ridge regression from $\operatorname  {MSE}_{\operatorname  L2}$ (5 points). }{4}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Relate the MSE objective for supervised regression to the cross-entropy objective: $\operatorname  {CE}({\bf  w}) = -\DOTSI \intop \ilimits@ _{\mathcal  {X}} p^*(x) \DOTSI \intop \ilimits@ _{\mathcal  {Y}} p^*(y|x) \qopname  \relax o{log}p_{{\bf  w}}(y|x) dy dx$, where $p^*$ is the true distribution of $Y$ and ${\bf  x}$, and $ p_{{\bf  w}}$ is the estimated distribution. Use the fact that \\ $\operatorname  {MSE}({\bf  w}) = \operatorname  {E} \left [ \left (Y - {\bf  x}^\top {\bf  w}\right )^2 \right ] = \DOTSI \intop \ilimits@ _{\mathcal  {X}} p^*(x) \DOTSI \intop \ilimits@ _{\mathcal  {Y}} p^*(y|x) (y - x^\top {\bf  w})^2 dy dx$. (5 points). }{5}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ We discussed how supervised and reinforcement learning problems are fundamentally different. Show how that difference impacts the choice of policy optimization objective in terms of cross entropy in the case of bandits. More specifically, formulate the policy optimization problem with a cross entropy objective, discuss the difference between this objective and the cross entropy objective for supervised regression defined above, and relate this difference to the fundamental difference between supervised and reinforcement learning problems (5 points). }{5}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {9}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ The expected policy gradient update for contextual bandit problem can be given as follows: $\operatorname  {E} \left [ - \qopname  \relax o{log}\pi _\theta \left ( A | X \right ) R \right ]$, where $X$ is a given context, $\pi _\theta $ is the parameterized policy distribution maintained by the agent, $A$ is the action chosen by the agent according to $\pi _\theta $, and $R$ is the subsequent reward. Show that this update can be derived by calculating the gradient of the cross entropy objective: $\operatorname  {CE}(\theta ) = - \DOTSI \intop \ilimits@ _{\mathcal  {X}} p(x) \DOTSI \intop \ilimits@ _{\mathcal  {A}} \pi _\theta (a|x) \qopname  \relax o{log}\pi ^*(a|x) da dx $, where $p$, which is not a function of $\theta $, is the distribution of context $X$. Also, $\pi ^*$ is the ideal policy distribution defined as the Boltzmann distribution $\pi ^*(a|x) = Z^{-1} e^{q(x,a) \tau ^{-1}}$, where $q(x,a) = \operatorname  {E} \left [ R | A = a, X = x \right ]$ is the contextual action value. (10 points). }{6}{section.9}}
\@writefile{toc}{\contentsline {section}{\numberline {10}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ We wrote a PyTorch code for a discrete action bandit problem. Consider a continuous action bandit problem, where actions are distributed normally: $A \sim N(\mu , \sigma ^2)$, and the reward is a quadratic function of the action: $R = - \left ( A - 10 \right )^2$. What is the optimal policy in terms of mean $\mu $ and standard deviation $\sigma $ (5 points)? Write a PyTorch code for learning the mean and the standard deviation using policy gradient. Attach the code here together with the curve for estimated mean and standard deviation over time for 100K time steps. The code should run without error in colab and bring the plots. (15 points, total 20). }{7}{section.10}}
\@writefile{lol}{\contentsline {lstlisting}{pg.py}{7}{lstlisting.-1}}
\@writefile{toc}{\contentsline {section}{\numberline {11}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ For discrete MDP, Derive the following Bellman equation for action value function from its definition: $q_\pi (s,a) = \operatorname  {E} \left [ G_t | S_t = s, A_t = a, A_k \sim \pi , \forall k > t \right ]$, where $S_t$ is the state at $t$, $A_t$ is the action at $t$, $G_t$ is the return after $t$, $\pi $ is the policy, $\gamma $ is the discount factor, $r(s,a) = \DOTSB \sum@ \slimits@ _r r \DOTSB \sum@ \slimits@ _{s'} p(s', r | s, a)$ and $p_\pi (s', a'|s, a) = \DOTSB \sum@ \slimits@ _r p(s', r| s, a)\pi (a'|s')$ (5 points). }{8}{section.11}}
\@writefile{toc}{\contentsline {section}{\numberline {12}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Write the above Bellman equation in matrix form (5 points). }{8}{section.12}}
\@writefile{toc}{\contentsline {section}{\numberline {13}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Consider a policy $\pi $ that is not optimal, that is $v_\pi (s) < v_{\pi ^*}(s), \exists s$. Also consider the greedy-policy operator $g$ that gives a policy $gq$ which is greedy with respect to a given function $q$, that is, $(g q)(s) = \qopname  \relax o{arg}\qopname  \relax m{max}_b q(s, b)$, assuming there is no tie. Show that, $v_{gq_\pi }(s) > v_\pi (s), \exists s$ (5 points). }{9}{section.13}}
\@writefile{toc}{\contentsline {section}{\numberline {14}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ As we have seen, the action value iteration method can be written as: ${\bf  q}_{k+1} = {\bf  q}_k - \alpha \left ( \left ({\bf  I} - \gamma {\bf  P}_{g{\bf  q}_k} \right ){\bf  q}_k - {\bf  r}\right )$, where ${\bf  q}$ is the estimate of action value in vector form, and the vector ${\bf  r}$ contains $\left [ {\bf  r}\right ]_{sa} = r(s,a)$ for different $s,a$. Also, the matrix ${\bf  P}_{g {\bf  q}_k}$ contains state-action-transition probabilities, $\left [ {\bf  P}_{g{\bf  q}_k} \right ]_{sa, s'a'} = p(s'|s, a) (g {\bf  q}_k)(a'|s')$, where next actions are taken under policy $g {\bf  q}_k$, which is the greedy policy with respect to ${\bf  q}_k$. Find the condition under which the action-value iteration method converges, and convince that the condition is satisfied in general. (5 points). }{9}{section.14}}
\@writefile{toc}{\contentsline {section}{\numberline {15}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Write the following state-value iteration method in matrix form: \\ $v_{k+1}(s) = v_k(s) + \alpha \left ( \qopname  \relax m{max}_b \left [ r(s, b) + \gamma \DOTSB \sum@ \slimits@ _{s'} p(s'|s, b) v_{k}(s') \right ] - v_k(s) \right )$ (5 points). }{10}{section.15}}
\@writefile{toc}{\contentsline {section}{\numberline {16}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Consider the two-state MDP example from lecture, where $\gamma =0.9$, reward $r(s,a)$ is 1 for state 1 and action 1, and zero for all other state action pairs, and the state transition probabilities $p(s'|s,a)$ are as follows: $p(0|0,0) = p(1|0,1) = p(0|1,0) = p(1|1,1)=1$, and zero for any other transition. (5 points). Now, consider a policy $\pi $ which takes action 0 at state 0 with probability 0.25 but takes the same action at state 1 with probability 0.75. Find $r_\pi (s) = \DOTSB \sum@ \slimits@ _a \pi (a|s)r(s,a)$, $p_\pi (s'|s) = \DOTSB \sum@ \slimits@ _a \pi (a|s)p(s'|s,a)$, and $v_\pi (s)$ for all $s$ and $s'$. Mention the method you used for finding $v_\pi $ or show calculation. (5 points). }{10}{section.16}}
\@writefile{toc}{\contentsline {section}{\numberline {17}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ For the same MDP, find $q_\pi $, mention the method you used for finding $q_\pi $ or show calculation, and describe $g q_\pi $, that is the greedy policy over $q_\pi $ for the policy $\pi $ described in the previous question (5 points). }{11}{section.17}}
\@writefile{toc}{\contentsline {section}{\numberline {18}\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \belowdisplayskip \abovedisplayskip \let \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ Consider the nonlinear function approximator for supervised regression $Y \approx {\bf  \theta }^\top \phi ({\bf  W}{\bf  x})$, where $Y$ is the target scalar output, ${\bf  x}$ is the vector input, ${\bf  \theta }$ is the output weight vector, $\bf  W$ is the input weight matrix, and $\phi $ is the element-wise activation function for the hidden layer. Consider the activation function to be sigmoid. Show the backpropagation update for input weights for this case in matrix form. (5 points). }{11}{section.18}}
