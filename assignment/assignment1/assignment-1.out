\BOOKMARK [1][-]{section.1}{Consider the supervised regression problem of predicting an output Y from a vector input x using a linear function with a vector parameter w. We found in the lecture that the Mean Squared Error \(MSE\) E [ \( Y - xw\)2 ] is a reasonable objective for this problem. Derive the Least-Squares \(LS\) method, defined below, from MSE \(5 points\). }{}% 1
\BOOKMARK [1][-]{section.2}{Continuing with the supervised regression problem, derive the Stochastic Gradient Descent \(SGD\) method, defined below, from MSE \(5 points\). }{}% 2
\BOOKMARK [1][-]{section.3}{According to the strong law of large numbers, sample mean is a consistent estimator of the expected value \(See Lemma 3, pg 18, of Mahmood 2017\). Use this fact to argue that the sample MSE 1t k=1t \( Yk - xkw\)2 objective is a consistent estimator of MSE \(5 points\). }{}% 3
\BOOKMARK [1][-]{section.4}{Similarly, argue that LS is a consistent estimator of the solution to MSE \(5 points\). }{}% 4
\BOOKMARK [1][-]{section.5}{Establish a relationship between MSE and the following: E x[ \( E Y|x[ Y] - xw\)2 ]. Note that MSE can be written as E [ \(Y - xw\)2 ] = E xE Y|x[ \(Y - xw\)2 ]. Also, the expectations E X and E Y|X can be elaborated as E X f\(X\) = X p*\(x\) f\(x\) dx and EY|X f\(Y\) = Y p*\(y|x\) f\(y\) dy \(5 points\). }{}% 5
\BOOKMARK [1][-]{section.6}{Objective functions serve many purposes. They bring specificity to the problem formulation, provide a criteria for evaluation and can be used as a basis for deriving new algorithms. Accordingly, the same problem may have different associated objectives. For example, the sample MSE is widely used to evaluate the performance of a learning method, but not all methods ensue from MSE. Consider the following objective MSEL2, which is a slight modification to MSE: MSEL2\(w\) = E [ \( Y - xw\)2 + ww]. Derive the following method know as ridge regression from MSEL2 \(5 points\). }{}% 6
\BOOKMARK [1][-]{section.7}{Relate the MSE objective for supervised regression to the cross-entropy objective: CE\(w\) = -X p*\(x\) Y p*\(y|x\) logpw\(y|x\) dy dx, where p* is the true distribution of Y and x, and \040pw is the estimated distribution. Use the fact that \040MSE\(w\) = E [ \(Y - xw\)2 ] = X p*\(x\) Y p*\(y|x\) \(y - xw\)2 dy dx. \(5 points\). }{}% 7
\BOOKMARK [1][-]{section.8}{We discussed how supervised and reinforcement learning problems are fundamentally different. Show how that difference impacts the choice of policy optimization objective in terms of cross entropy in the case of bandits. More specifically, formulate the policy optimization problem with a cross entropy objective, discuss the difference between this objective and the cross entropy objective for supervised regression defined above, and relate this difference to the fundamental difference between supervised and reinforcement learning problems \(5 points\). }{}% 8
\BOOKMARK [1][-]{section.9}{The expected policy gradient update for contextual bandit problem can be given as follows: E [ - log\( A | X \) R ], where X is a given context, \040is the parameterized policy distribution maintained by the agent, A is the action chosen by the agent according to , and R is the subsequent reward. Show that this update can be derived by calculating the gradient of the cross entropy objective: CE\(\) = - X p\(x\) A \(a|x\) log*\(a|x\) da dx , where p, which is not a function of , is the distribution of context X. Also, * is the ideal policy distribution defined as the Boltzmann distribution *\(a|x\) = Z-1 eq\(x,a\) -1, where q\(x,a\) = E [ R | A = a, X = x ] is the contextual action value. \(10 points\). }{}% 9
\BOOKMARK [1][-]{section.10}{We wrote a PyTorch code for a discrete action bandit problem. Consider a continuous action bandit problem, where actions are distributed normally: A N\(, 2\), and the reward is a quadratic function of the action: R = - \( A - 10 \)2. What is the optimal policy in terms of mean \040and standard deviation \040\(5 points\)? Write a PyTorch code for learning the mean and the standard deviation using policy gradient. Attach the code here together with the curve for estimated mean and standard deviation over time for 100K time steps. The code should run without error in colab and bring the plots. \(15 points, total 20\). }{}% 10
\BOOKMARK [1][-]{section.11}{For discrete MDP, Derive the following Bellman equation for action value function from its definition: q\(s,a\) = E [ Gt | St = s, At = a, Ak , k > t ], where St is the state at t, At is the action at t, Gt is the return after t, \040is the policy, \040is the discount factor, r\(s,a\) = r r s' p\(s', r | s, a\) and p\(s', a'|s, a\) = r p\(s', r| s, a\)\(a'|s'\) \(5 points\). }{}% 11
\BOOKMARK [1][-]{section.12}{Write the above Bellman equation in matrix form \(5 points\). }{}% 12
\BOOKMARK [1][-]{section.13}{Consider a policy \040that is not optimal, that is v\(s\) < v*\(s\), s. Also consider the greedy-policy operator g that gives a policy gq which is greedy with respect to a given function q, that is, \(g q\)\(s\) = argmaxb q\(s, b\), assuming there is no tie. Show that, vgq\(s\) > v\(s\), s \(5 points\). }{}% 13
\BOOKMARK [1][-]{section.14}{As we have seen, the action value iteration method can be written as: qk+1 = qk - \( \(I - Pgqk \)qk - r\), where q is the estimate of action value in vector form, and the vector r contains [ r]sa = r\(s,a\) for different s,a. Also, the matrix Pg qk contains state-action-transition probabilities, [ Pgqk ]sa, s'a' = p\(s'|s, a\) \(g qk\)\(a'|s'\), where next actions are taken under policy g qk, which is the greedy policy with respect to qk. Find the condition under which the action-value iteration method converges, and convince that the condition is satisfied in general. \(5 points\). }{}% 14
\BOOKMARK [1][-]{section.15}{Write the following state-value iteration method in matrix form: \040vk+1\(s\) = vk\(s\) + \( maxb [ r\(s, b\) + s' p\(s'|s, b\) vk\(s'\) ] - vk\(s\) \) \(5 points\). }{}% 15
\BOOKMARK [1][-]{section.16}{Consider the two-state MDP example from lecture, where =0.9, reward r\(s,a\) is 1 for state 1 and action 1, and zero for all other state action pairs, and the state transition probabilities p\(s'|s,a\) are as follows: p\(0|0,0\) = p\(1|0,1\) = p\(0|1,0\) = p\(1|1,1\)=1, and zero for any other transition. \(5 points\). Now, consider a policy \040which takes action 0 at state 0 with probability 0.25 but takes the same action at state 1 with probability 0.75. Find r\(s\) = a \(a|s\)r\(s,a\), p\(s'|s\) = a \(a|s\)p\(s'|s,a\), and v\(s\) for all s and s'. Mention the method you used for finding v or show calculation. \(5 points\). }{}% 16
\BOOKMARK [1][-]{section.17}{For the same MDP, find q, mention the method you used for finding q or show calculation, and describe g q, that is the greedy policy over q for the policy \040described in the previous question \(5 points\). }{}% 17
\BOOKMARK [1][-]{section.18}{Consider the nonlinear function approximator for supervised regression Y \(Wx\), where Y is the target scalar output, x is the vector input, \040is the output weight vector, W is the input weight matrix, and \040is the element-wise activation function for the hidden layer. Consider the activation function to be sigmoid. Show the backpropagation update for input weights for this case in matrix form. \(5 points\). }{}% 18
